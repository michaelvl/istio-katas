!INCLUDE "markdown-source/meta.md"
[//]: # (Tags: #metrics #prometheus-annotations #sidecar-injection)

# A Tour of the Istio Metrics

This exercise will demonstrate how to use metrics from Istio together with
Prometheus. We will also see how application metrics and Istio metrics work
together.

Note: This exercise assumes an Istio deployment with `enablePrometheusMerge` enabled.

Deploy the sentences application:

```console
kubectl apply -f deploy/metrics/sentences.yaml
```

Execute `kubectl get pods` and observe that we have one container per POD:

```
NAME                         READY   STATUS    RESTARTS   AGE
age-657d4d9678-q8h7d         1/1     Running   0          3s
name-86969f7468-4qfmp        1/1     Running   0          3s
sentences-779767c659-mlcm9   1/1     Running   0          4s
```

Next, execute `scripts/loop-query.sh` to see the application is running. This will also
update both Istio and sentences application metrics.

To retrieve metrics from a POD in the sentences application we can query the
nodeport mapped to the metrics port 8000:

```console
curl -s <Any node IP>:<node port for POD port 8000>/metrics | grep sentence_requests_total
```

This will return something like:

```
# HELP sentence_requests_total Number of requests
# TYPE sentence_requests_total counter
sentence_requests_total{type="name"} 584.0
```

This showns that the POD had received `584` requests from the `loop-query.sh`
script when we fetched metrics.

The deployed version of the sentences application have Istio sidecar injection
disabled. This is done through annotations - investigate the yaml file and
observe the use of the `sidecar.istio.io/inject` annotation:

```
      annotations:
        sidecar.istio.io/inject: 'false'
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8000'
        prometheus.io/path: '/metrics'
```

Also note the Prometheus annotations that informs Prometheus, that this POD can
be scraped for metrics on port `8000` and path `/metrics` - similar to what we
just did manually.

## Enable Sidecar Injection

Next, we will allow Istio to inject a sidecar. Re-deploy the sentences
application without the annotation that disables sidecar injection:

```console
cat deploy/metrics/sentences.yaml |grep -v inject | kubectl apply -f -
```

If we run `kubectl get pods` now, we will see that we have two containers per
POD (it may take a few seconds for the old PODs to terminate).

Next, observe the values of the Prometheus annotations:

```console
kubectl describe pod -l mode=sentence | head -n 30
```

The result should look like this:

```
Annotations:  prometheus.io/path: /metrics
              prometheus.io/port: 8000
              prometheus.io/scrape: true
              prometheus.istio.io/merge-metrics: false
```

So there is no change in how Prometheus will scrape POD metrics - it will still
use port `8000` which is handled by the sentences application container.

What about the Istio metrics from the sidecar?

Istio supports merging Prometheus metrics from the application and the sidecar
into a single scrape endpoint, however this has been disabled with the
annotation `prometheus.istio.io/merge-metrics` (see above).

Re-deploy the sentences application with this annotation removed as well:

```console
cat deploy/metrics/sentences.yaml |egrep -v 'inject|merge-metrics' | kubectl apply -f -
```

If we now inspect the POD annotations as above, we see:

```
Annotations:  prometheus.io/path: /stats/prometheus
              prometheus.io/port: 15020
              prometheus.io/scrape: true
```

i.e. the metrics scrape endpoint has moved from the application to the sidecar.

## Fetch Istio Metrics

> The following illustrate how to fetch the merged metrics using the command
> line. If you have Prometheus or e.g. Grafana deployed, you could also use one of
> those to do the queries shown here.

First, list PODs to get their cluster IP:

```console
kubectl get pods -o wide
```

Next, deploy a test tool (if you prefer, you can use any other tool that have `curl` and `grep`):

```console
kubectl create deploy multitool --image wbitt/network-multitool
kubectl exec -it `kubectl get pods -l app=multitool --output=jsonpath={.items..metadata.name}` -- bash
```

Inside the test tool, run `curl` against the metrics scrape endpoint defined by
the POD annotations - insert the <POD IP> found above:

```console
curl -s <POD IP>:15020/stats/prometheus | grep requests_total
```

the result of which should look somewhat like the following for e.g. the `name` service (output slightly edited for clarity):

```
istio_requests_total{response_code="200",
                     source_workload="sentences",
                     source_version="unknown",
                     destination_workload="name",
                     destination_version="unknown"}   265
sentence_requests_total{type="name"}                  265
```

Note, that we both see a `sentence_requests_total` metric and an
`istio_requests_total` metric - the former generated by the sentences
application `name` service and the other by the Istio sidecar. They should show
the same numeric value, however, since the Istio metric contain additional
labels for e.g. source and destination of requests there could be differences
with the request count spread out on differently labelled `istio_requests_total`
metrics.

The labels `source_workload`, `destination_workload`, `source_version` etc. is
the primary information Kiali use to dynamically build application graphs and
versioned graphs. See this link for more information on how [Kiali use
Prometheus metrics](https://kiali.io/documentation/latest/faq/#prom-metrics)

## Cleanup

```console
kubectl delete -f deploy/metrics/sentences.yaml
kubectl delete deploy multitool
```
